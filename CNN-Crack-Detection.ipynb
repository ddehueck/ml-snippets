{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import numpy as np\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_WORKERS = 1\n",
    "NUM_EPOCHS = 100\n",
    "np.random.seed(42)\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder('./sdnet-data/pavement', transform=transform)\n",
    "\n",
    "classes = ('Cracked', 'Not Cracked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split: 70% train | 15% validation | 15% test\n",
    "\"\"\"\n",
    "dataset_size = len(dataset)\n",
    "train_split = int(np.floor(0.70 * dataset_size))\n",
    "val_split = train_split + int(np.floor(0.15 * dataset_size))\n",
    "\n",
    "indices = list(range(dataset_size))\n",
    "np.random.shuffle(indices)\n",
    "train_idxs, val_idxs, test_idxs = indices[:train_split], indices[train_split:val_split], indices[val_split:]\n",
    "\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idxs)\n",
    "val_sampler = SubsetRandomSampler(val_idxs)\n",
    "test_sampler = SubsetRandomSampler(test_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load data\n",
    "\"\"\"\n",
    "\n",
    "trainloader = DataLoader(\n",
    "    dataset, batch_size=BATCH_SIZE, sampler=train_sampler,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "valloader = DataLoader(\n",
    "    dataset, batch_size=BATCH_SIZE, sampler=val_sampler,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "testloader = DataLoader(\n",
    "    dataset, batch_size=BATCH_SIZE, sampler=test_sampler,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3651 examples total\n",
      "1806 or 49.47% are not cracked\n",
      "1845 or 50.53% are cracked\n",
      "\n",
      "There are 782 examples total\n",
      "391 or 50.00% are not cracked\n",
      "391 or 50.00% are cracked\n",
      "\n",
      "There are 783 examples total\n",
      "411 or 52.49% are not cracked\n",
      "372 or 47.51% are cracked\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Examine class balances in splits\n",
    "\"\"\"\n",
    "\n",
    "def view_class_balance(dataloader):\n",
    "    total = 0\n",
    "    not_cracked_total = 0\n",
    "    for data in dataloader:\n",
    "        images, labels = data\n",
    "        total += len(labels)\n",
    "        not_cracked_total += sum(labels).item()\n",
    "\n",
    "    print(\"There are %i examples total\" % total)\n",
    "    print(\"%i or %.2f%% are not cracked\" % (not_cracked_total, (not_cracked_total/total)*100))\n",
    "    print(\"%i or %.2f%% are cracked\" % (total - not_cracked_total, ((total - not_cracked_total)/total)*100))\n",
    "    print()\n",
    "    \n",
    "view_class_balance(trainloader)\n",
    "view_class_balance(valloader)\n",
    "view_class_balance(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Helper functions to view images.\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize - corresponds to transformation above.\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images[0]))\n",
    "print('Labeled as:', classes[labels[0].item()])\n",
    "\"\"\"\n",
    "h = models.resnet18()\n",
    "h.fc = nn.Linear(512, 2)\n",
    "h.conv1\n",
    "print(h.conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Define Model\n",
    "\"\"\"\n",
    "\n",
    "class DetectionModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(DetectionModel, self).__init__()\n",
    "        self.resnet = models.resnet18()\n",
    "        self.resnet.fc = nn.Linear(512, 2)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Instantiate a MNISTModel to be trained\n",
    "\"\"\"\n",
    "\n",
    "model = DetectionModel().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | loss: 0.583\n",
      "Epoch: 2 | loss: 0.485\n",
      "Epoch: 3 | loss: 0.426\n",
      "Epoch: 4 | loss: 0.346\n",
      "Epoch: 5 | loss: 0.245\n",
      "Epoch: 6 | loss: 0.193\n",
      "Epoch: 7 | loss: 0.154\n",
      "Epoch: 8 | loss: 0.086\n",
      "Epoch: 9 | loss: 0.085\n",
      "Epoch: 10 | loss: 0.123\n",
      "Epoch: 11 | loss: 0.071\n",
      "Epoch: 12 | loss: 0.068\n",
      "Epoch: 13 | loss: 0.054\n",
      "Epoch: 14 | loss: 0.042\n",
      "Epoch: 15 | loss: 0.051\n",
      "Epoch: 16 | loss: 0.095\n",
      "Epoch: 17 | loss: 0.041\n",
      "Epoch: 18 | loss: 0.029\n",
      "Epoch: 19 | loss: 0.036\n",
      "Epoch: 20 | loss: 0.052\n",
      "Epoch: 21 | loss: 0.028\n",
      "Epoch: 22 | loss: 0.021\n",
      "Epoch: 23 | loss: 0.023\n",
      "Epoch: 24 | loss: 0.024\n",
      "Epoch: 25 | loss: 0.051\n",
      "Epoch: 26 | loss: 0.029\n",
      "Epoch: 27 | loss: 0.019\n",
      "Epoch: 28 | loss: 0.071\n",
      "Epoch: 29 | loss: 0.020\n",
      "Epoch: 30 | loss: 0.043\n",
      "Epoch: 31 | loss: 0.025\n",
      "Epoch: 32 | loss: 0.013\n",
      "Epoch: 33 | loss: 0.011\n",
      "Epoch: 34 | loss: 0.022\n",
      "Epoch: 35 | loss: 0.093\n",
      "Epoch: 36 | loss: 0.022\n",
      "Epoch: 37 | loss: 0.015\n",
      "Epoch: 38 | loss: 0.017\n",
      "Epoch: 39 | loss: 0.018\n",
      "Epoch: 40 | loss: 0.052\n",
      "Epoch: 41 | loss: 0.006\n",
      "Epoch: 42 | loss: 0.002\n",
      "Epoch: 43 | loss: 0.003\n",
      "Epoch: 44 | loss: 0.012\n",
      "Epoch: 45 | loss: 0.045\n",
      "Epoch: 46 | loss: 0.010\n",
      "Epoch: 47 | loss: 0.024\n",
      "Epoch: 48 | loss: 0.014\n",
      "Epoch: 49 | loss: 0.027\n",
      "Epoch: 50 | loss: 0.021\n",
      "Epoch: 51 | loss: 0.006\n",
      "Epoch: 52 | loss: 0.014\n",
      "Epoch: 53 | loss: 0.008\n",
      "Epoch: 54 | loss: 0.013\n",
      "Epoch: 55 | loss: 0.020\n",
      "Epoch: 56 | loss: 0.042\n",
      "Epoch: 57 | loss: 0.030\n",
      "Epoch: 58 | loss: 0.007\n",
      "Epoch: 59 | loss: 0.009\n",
      "Epoch: 60 | loss: 0.052\n",
      "Epoch: 61 | loss: 0.016\n",
      "Epoch: 62 | loss: 0.004\n",
      "Epoch: 63 | loss: 0.015\n",
      "Epoch: 64 | loss: 0.002\n",
      "Epoch: 65 | loss: 0.003\n",
      "Epoch: 66 | loss: 0.033\n",
      "Epoch: 67 | loss: 0.013\n",
      "Epoch: 68 | loss: 0.006\n",
      "Epoch: 69 | loss: 0.033\n",
      "Epoch: 70 | loss: 0.012\n",
      "Epoch: 71 | loss: 0.019\n",
      "Epoch: 72 | loss: 0.003\n",
      "Epoch: 73 | loss: 0.012\n",
      "Epoch: 74 | loss: 0.005\n",
      "Epoch: 75 | loss: 0.026\n",
      "Epoch: 76 | loss: 0.004\n",
      "Epoch: 77 | loss: 0.001\n",
      "Epoch: 78 | loss: 0.002\n",
      "Epoch: 79 | loss: 0.025\n",
      "Epoch: 80 | loss: 0.023\n",
      "Epoch: 81 | loss: 0.009\n",
      "Epoch: 82 | loss: 0.006\n",
      "Epoch: 83 | loss: 0.003\n",
      "Epoch: 84 | loss: 0.002\n",
      "Epoch: 85 | loss: 0.002\n",
      "Epoch: 86 | loss: 0.006\n",
      "Epoch: 87 | loss: 0.044\n",
      "Epoch: 88 | loss: 0.010\n",
      "Epoch: 89 | loss: 0.002\n",
      "Epoch: 90 | loss: 0.001\n",
      "Epoch: 91 | loss: 0.001\n",
      "Epoch: 92 | loss: 0.001\n",
      "Epoch: 93 | loss: 0.001\n",
      "Epoch: 94 | loss: 0.002\n",
      "Epoch: 95 | loss: 0.000\n",
      "Epoch: 96 | loss: 0.000\n",
      "Epoch: 97 | loss: 0.000\n",
      "Epoch: 98 | loss: 0.000\n",
      "Epoch: 99 | loss: 0.005\n",
      "Epoch: 100 | loss: 0.050\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train model\n",
    "\"\"\"\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(inputs)\n",
    "        loss = criterion(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    print('Epoch: %d | loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 77 %\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test model\n",
    "\"\"\"\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in valloader:\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        preds = model(inputs)\n",
    "        _, predicted = torch.max(preds.data, 1) # max of logits\n",
    "        \n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
